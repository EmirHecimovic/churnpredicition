{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3d9f268",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df51e123",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will walk through the process of building a churn prediction model. Our primary focus is to identify customers who are likely to stop using a service, which can help businesses strategize retention efforts. We will undertake the following steps:\n",
    "\n",
    "1. Data Exploration and Insights\n",
    "2. Data Preprocessing\n",
    "3. Building Baseline Models\n",
    "4. Oversampling and Undersampling Techniques\n",
    "5. Hyperparameter Optimization\n",
    "6. Feature Importance and Selection\n",
    "7. Bagging to Reduce Overfitting\n",
    "8. Conclusion and Final Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1301423",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Loading the Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5a825f5",
   "metadata": {},
   "source": [
    "We begin by loading the dataset. The head() function provides a snapshot of the first few rows, helping us get a quick overview of the data columns and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/mnt/data/task_data_churned.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "128972c0",
   "metadata": {},
   "source": [
    "\n",
    "After loading the data, we observed that it contains several features and a target variable `churned_status` indicating whether a user has churned or not. It's essential to understand the data distribution and patterns before modeling, as this can inform our preprocessing and modeling steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fd401f1",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Cleaning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4c3b6a3",
   "metadata": {},
   "source": [
    "It's essential to ensure that our data doesn't have missing values, as they can impact the modeling process. Additionally, understanding the data types of each column helps in deciding which preprocessing steps are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display columns with missing values and their count\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be3da8e9",
   "metadata": {},
   "source": [
    "The dataset contains missing values in the following columns:\n",
    "\n",
    "- action_gps_tracking: 1626 missing values\n",
    "- action_screenshots: 1458 missing values\n",
    "- action_create_custom_field: 2059 missing values\n",
    "- country: 84 missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ae6da45",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb29e601",
   "metadata": {},
   "source": [
    "We visualize the distribution of the target variable, churned_status, to understand the balance between the classes. Such visualizations are crucial in highlighting potential class imbalances which can affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06952429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable 'churned_status'\n",
    "target_distribution = data['churned_status'].value_counts()\n",
    "\n",
    "# Display the distribution\n",
    "target_distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Selecting a subset of columns for visualization\n",
    "cols_to_visualize = ['ws_users_activated', 'ws_users_deactivated', 'ws_users_invited', 'action_create_project', 'revenue']\n",
    "\n",
    "# Plotting the distribution of selected columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(cols_to_visualize, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(data[col], bins=50, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65880fee",
   "metadata": {},
   "source": [
    "The distribution of the churned_status shows:\n",
    "\n",
    "- 1703 instances where the churned status is \"No\" (meaning the users did not churn).\n",
    "- 799 instances where the churned status is \"Yes\" (meaning the users churned).\n",
    "\n",
    "This indicates that the dataset is somewhat imbalanced, with more instances of non-churned users compared to churned users. This imbalance will need to be addressed during modeling, as it can lead to models that are biased towards predicting the majority (non-churned) class.\n",
    "\n",
    "From the data description, we can draw several additional insights:\n",
    "\n",
    "1. There is an almost even split between churned and non-churned users.\n",
    "2. The average age of users is around 28 years.\n",
    "3. Users, on average, spend about 4.5 minutes on the platform.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afe28e57",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c73056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Impute missing values with 0\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Check if there are any more missing values\n",
    "remaining_missing = data.isnull().sum().sum()\n",
    "remaining_missing\n",
    "\n",
    "# One-hot encode the 'country' column\n",
    "data_encoded = pd.get_dummies(data, columns=['country'], drop_first=True)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = data_encoded.drop('churned_status', axis=1)\n",
    "y = data_encoded['churned_status']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shape of the splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7b4f2c",
   "metadata": {},
   "source": [
    "Models require numerical input, so categorical data is encoded into a numerical format using \"one-hot encoding\". We then split the dataset into training and test sets. The training set is used to train the model, while the test set helps evaluate model performance.\n",
    "\n",
    "The data has been split into following training and testing sets:\n",
    "\n",
    "- Training features (Xtrain​): 2001 samples with 179 features\n",
    "- Testing features (Xtest): 501 samples with 179 features\n",
    "- Training target (Ytrain): 2001 samples\n",
    "- Testing target (Ytest)​: 501 samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78f058d9",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa0dda4",
   "metadata": {},
   "source": [
    "\n",
    "Establishing baseline models is an essential step in the modeling process. These models provide a benchmark performance, which can be used as a reference when experimenting with more complex models or techniques. In this section, we'll build and evaluate three baseline models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Gradient Boosting\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a7f6bdd",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method that predicts the probability of a binary outcome. We train the model using the training data and then make predictions on the test set. Metrics like accuracy, precision, recall, and F1-score are computed to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "precision_logreg = precision_score(y_test, y_pred_logreg, pos_label=\"Yes\")\n",
    "recall_logreg = recall_score(y_test, y_pred_logreg, pos_label=\"Yes\")\n",
    "report_logreg = classification_report(y_test, y_pred_logreg)\n",
    "confusion_logreg = confusion_matrix(y_test, y_pred_logreg)\n",
    "\n",
    "accuracy_logreg, precision_logreg, recall_logreg, report_logreg, confusion_logreg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e249158a",
   "metadata": {},
   "source": [
    "From this, we can observe:\n",
    "\n",
    "- The model correctly predicted 287 non-churned users and 65 churned users.\n",
    "- However, it misclassified 95 users as non-churned when they actually churned and 54 users as churned when they didn't"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a925736",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Random Forest\n",
    "\n",
    "Random Forest is an ensemble method that combines multiple decision trees to produce a more accurate and robust prediction. It is particularly effective for datasets with a large number of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, pos_label=\"Yes\")\n",
    "recall_rf = recall_score(y_test, y_pred_rf, pos_label=\"Yes\")\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "confusion_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "accuracy_rf, precision_rf, recall_rf, report_rf, confusion_rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17d0060",
   "metadata": {},
   "source": [
    "From this, we can observe:\n",
    "\n",
    "- The Random Forest model correctly predicted 308 non-churned users and 70 churned users.\n",
    "- It misclassified 90 users as non-churned when they actually churned and 33 users as churned when they didn't.\n",
    "\n",
    "Comparing with the Logistic Regression model, the Random Forest model has improved in terms of accuracy and precision, but recall is still a concern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ce28d8e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Gradient Boosting\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2206c00",
   "metadata": {},
   "source": [
    "In this step, we initialize a Gradient Boosting classifier, train it using the training data, and then make predictions on the test set. Once the model is trained and predictions are made, it is essential to evaluate its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "precision_gb = precision_score(y_test, y_pred_gb, pos_label=\"Yes\")\n",
    "recall_gb = recall_score(y_test, y_pred_gb, pos_label=\"Yes\")\n",
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "confusion_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "accuracy_gb, precision_gb, recall_gb, report_gb, confusion_gb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7891de50",
   "metadata": {},
   "source": [
    "\n",
    "## Tabular Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b54f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics for each model\n",
    "logistic_metrics = {\n",
    "    'Precision': [0.75, 0.55, 0.65, 0.69],\n",
    "    'Recall': [0.84, 0.41, 0.62, 0.70],\n",
    "    'F1-Score': [0.79, 0.47, 0.63, 0.69],\n",
    "    'Support': [341, 160, 501, 501]\n",
    "}\n",
    "\n",
    "random_forest_metrics = {\n",
    "    'Precision': [0.77, 0.68, 0.73, 0.74],\n",
    "    'Recall': [0.90, 0.44, 0.67, 0.75],\n",
    "    'F1-Score': [0.83, 0.53, 0.68, 0.74],\n",
    "    'Support': [341, 160, 501, 501]\n",
    "}\n",
    "\n",
    "gradient_boosting_metrics = {\n",
    "    'Precision': [0.76, 0.61, 0.69, 0.71],\n",
    "    'Recall': [0.87, 0.43, 0.65, 0.73],\n",
    "    'F1-Score': [0.81, 0.50, 0.66, 0.71],\n",
    "    'Support': [341, 160, 501, 501]\n",
    "}\n",
    "\n",
    "# Convert metrics to DataFrames\n",
    "logistic_df = pd.DataFrame(logistic_metrics, index=['No', 'Yes', 'Macro Avg', 'Weighted Avg'])\n",
    "random_forest_df = pd.DataFrame(random_forest_metrics, index=['No', 'Yes', 'Macro Avg', 'Weighted Avg'])\n",
    "gradient_boosting_df = pd.DataFrame(gradient_boosting_metrics, index=['No', 'Yes', 'Macro Avg', 'Weighted Avg'])\n",
    "\n",
    "# Display the metrics\n",
    "logistic_df, random_forest_df, gradient_boosting_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42d296e",
   "metadata": {},
   "source": [
    "We compared three machine learning models—Logistic Regression, Random Forest, and Gradient Boosting—for predicting customer churn. The Random Forest model exhibited the highest accuracy at 75%, closely followed by Gradient Boosting at 73%, and Logistic Regression at 70%. While all three models showcased competitive performance, the Random Forest model balanced accuracy with a good recall rate, making it slightly more suited for our churn prediction objective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d678d354",
   "metadata": {},
   "source": [
    "# Advanced Techniques and Model Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dda585d",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we delve deeper into techniques that can potentially improve the performance of our models, especially when dealing with imbalanced datasets. We'll explore:\n",
    "\n",
    "1. Oversampling and undersampling techniques to balance the class distribution.\n",
    "2. Evaluating model performance with the oversampled data.\n",
    "3. Investigating feature importance to understand which features drive the predictions.\n",
    "4. Fine-tuning the model to optimize its performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5e3b932",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Oversampling and Undersampling\n",
    "\n",
    "As mentioned before, class imbalance can lead to biased models since they might be overly influenced by the majority class. To address this, we used oversampling to artificially increase the number of samples in the minority class (churned customers) and undersampling to reduce the number of samples in the majority class. By balancing the classes, we aim to improve the model's ability to predict both churned and non-churned customers accurately.\n",
    "\n",
    "After balancing the dataset, it is crucial to evaluate how the model performs with this new data. This step will provide insights into whether the balancing technique has a positive or negative impact on the model's predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Oversampling the minority class\n",
    "churn_yes = data[data['churned_status'] == 'Yes']\n",
    "churn_no = data[data['churned_status'] == 'No']\n",
    "\n",
    "churn_yes_oversampled = resample(churn_yes, replace=True, n_samples=len(churn_no), random_state=42)\n",
    "oversampled_data = pd.concat([churn_no, churn_yes_oversampled])\n",
    "\n",
    "# Undersampling the majority class\n",
    "churn_no_undersampled = resample(churn_no, replace=False, n_samples=len(churn_yes), random_state=42)\n",
    "undersampled_data = pd.concat([churn_yes, churn_no_undersampled])\n",
    "\n",
    "# One-hot encode the 'country' column for the oversampled and undersampled data\n",
    "oversampled_data_encoded = pd.get_dummies(oversampled_data, columns=['country'], drop_first=True)\n",
    "undersampled_data_encoded = pd.get_dummies(undersampled_data, columns=['country'], drop_first=True)\n",
    "\n",
    "X_oversampled = oversampled_data_encoded.drop('churned_status', axis=1)\n",
    "y_oversampled = oversampled_data_encoded['churned_status']\n",
    "\n",
    "X_undersampled = undersampled_data_encoded.drop('churned_status', axis=1)\n",
    "y_undersampled = undersampled_data_encoded['churned_status']\n",
    "\n",
    "# Splitting and scaling the oversampled data\n",
    "X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "X_train_over = scaler.fit_transform(X_train_over)\n",
    "X_test_over = scaler.transform(X_test_over)\n",
    "\n",
    "# Training and evaluating the Random Forest on oversampled data\n",
    "rf_over_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_over_model.fit(X_train_over, y_train_over)\n",
    "rf_over_predictions = rf_over_model.predict(X_test_over)\n",
    "\n",
    "rf_over_accuracy = accuracy_score(y_test_over, rf_over_predictions)\n",
    "rf_over_precision = precision_score(y_test_over, rf_over_predictions, pos_label=\"Yes\")\n",
    "rf_over_recall = recall_score(y_test_over, rf_over_predictions, pos_label=\"Yes\")\n",
    "rf_over_f1 = f1_score(y_test_over, rf_over_predictions, pos_label=\"Yes\")\n",
    "\n",
    "# Splitting and scaling the undersampled data\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_undersampled, y_undersampled, test_size=0.2, random_state=42)\n",
    "X_train_under = scaler.fit_transform(X_train_under)\n",
    "X_test_under = scaler.transform(X_test_under)\n",
    "\n",
    "# Training and evaluating the Random Forest on undersampled data\n",
    "rf_under_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_under_model.fit(X_train_under, y_train_under)\n",
    "rf_under_predictions = rf_under_model.predict(X_test_under)\n",
    "\n",
    "rf_under_accuracy = accuracy_score(y_test_under, rf_under_predictions)\n",
    "rf_under_precision = precision_score(y_test_under, rf_under_predictions, pos_label=\"Yes\")\n",
    "rf_under_recall = recall_score(y_test_under, rf_under_predictions, pos_label=\"Yes\")\n",
    "rf_under_f1 = f1_score(y_test_under, rf_under_predictions, pos_label=\"Yes\")\n",
    "\n",
    "rf_over_accuracy, rf_over_precision, rf_over_recall, rf_over_f1, rf_under_accuracy, rf_under_precision, rf_under_recall, rf_under_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e543a40e",
   "metadata": {},
   "source": [
    "Random Forest on Oversampled Data:\n",
    "\n",
    "- Accuracy: 87.39%\n",
    "- Precision (for churned status \"Yes\"): 82.51%\n",
    "- Recall (for churned status \"Yes\"): 91.59%\n",
    "- F1-Score: 86.81%\n",
    "\n",
    "Random Forest on Undersampled Data:\n",
    "\n",
    "- Accuracy: 70.00%\n",
    "- Precision (for churned status \"Yes\"): 73.65%\n",
    "- Recall (for churned status \"Yes\"): 65.66%\n",
    "- F1-Score: 69.43%\n",
    "\n",
    "From the results, we can observe that the Random Forest model trained on the oversampled data achieves a high accuracy, precision, and recall. This model effectively identifies a large percentage of churned users (indicated by the high recall). The model trained on the undersampled data has a decent accuracy and precision, but a slightly lower recall compared to the oversampled model. \n",
    "\n",
    "However, we would like to perform a robust evaluation of the model trained on the oversampled dataset and check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the oversampled training set\n",
    "rf_over_train_predictions = rf_over_model.predict(X_train_over)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "rf_over_train_accuracy = accuracy_score(y_train_over, rf_over_train_predictions)\n",
    "rf_over_train_precision = precision_score(y_train_over, rf_over_train_predictions, pos_label=\"Yes\")\n",
    "rf_over_train_recall = recall_score(y_train_over, rf_over_train_predictions, pos_label=\"Yes\")\n",
    "rf_over_train_f1 = f1_score(y_train_over, rf_over_train_predictions, pos_label=\"Yes\")\n",
    "\n",
    "rf_over_train_accuracy, rf_over_train_precision, rf_over_train_recall, rf_over_train_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbe19a0c",
   "metadata": {},
   "source": [
    "The model's performance on the oversampled training data is as follows:\n",
    "\n",
    "- Training Accuracy: 100%\n",
    "- Training Precision: 100%\n",
    "- Training Recall: 100%\n",
    "- Training F1-Score: 100%\n",
    "\n",
    "The model has achieved perfect scores on the training data, which is a strong indication of overfitting. Let us use RandomizedSearchCV to fine-tuning the parameters of the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47377404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameters\n",
    "param_dist_quick = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['auto']\n",
    "}\n",
    "\n",
    "# Perform a randomized search\n",
    "random_search_quick = RandomizedSearchCV(rf_over_model, param_distributions=param_dist_quick, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search_quick.fit(X_train_over, y_train_over)\n",
    "\n",
    "# Get the best parameters from the randomized search\n",
    "best_params_quick = random_search_quick.best_params_\n",
    "best_params_quick\n",
    "\n",
    "# Train the model using the best parameters\n",
    "rf_optimized = RandomForestClassifier(**best_params_quick, random_state=42)\n",
    "rf_optimized.fit(X_train_over, y_train_over)\n",
    "\n",
    "# Predict on the test set\n",
    "rf_optimized_predictions = rf_optimized.predict(X_test_over)\n",
    "\n",
    "# Evaluate the optimized model on the test set\n",
    "rf_optimized_accuracy = accuracy_score(y_test_over, rf_optimized_predictions)\n",
    "rf_optimized_precision = precision_score(y_test_over, rf_optimized_predictions, pos_label=\"Yes\")\n",
    "rf_optimized_recall = recall_score(y_test_over, rf_optimized_predictions, pos_label=\"Yes\")\n",
    "rf_optimized_f1 = f1_score(y_test_over, rf_optimized_predictions, pos_label=\"Yes\")\n",
    "\n",
    "rf_optimized_accuracy, rf_optimized_precision, rf_optimized_recall, rf_optimized_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44cf74ac",
   "metadata": {},
   "source": [
    "Random Forest model with optimized hyperparameters on the test set (oversampled data):\n",
    "\n",
    "- Accuracy: 83.72%\n",
    "- Precision (for churned status \"Yes\"): 76.90%\n",
    "- Recall (for churned status \"Yes\"): 91.59%\n",
    "- F1-Score: 83.60%\n",
    "\n",
    "The accuracy and precision of the optimized model are slightly lower than the previous model. The recall remains consistent. The decrease in performance might be because the optimized model is more regularized and less prone to overfitting. Let us check the performance of the optimized Random Forest model on the oversampled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the oversampled training set for the optimized model\n",
    "rf_optimized_train_predictions = rf_optimized.predict(X_train_over)\n",
    "\n",
    "# Evaluate the optimized model on the training set\n",
    "rf_optimized_train_accuracy = accuracy_score(y_train_over, rf_optimized_train_predictions)\n",
    "rf_optimized_train_precision = precision_score(y_train_over, rf_optimized_train_predictions, pos_label=\"Yes\")\n",
    "rf_optimized_train_recall = recall_score(y_train_over, rf_optimized_train_predictions, pos_label=\"Yes\")\n",
    "rf_optimized_train_f1 = f1_score(y_train_over, rf_optimized_train_predictions, pos_label=\"Yes\")\n",
    "\n",
    "rf_optimized_train_accuracy, rf_optimized_train_precision, rf_optimized_train_recall, rf_optimized_train_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "755ee364",
   "metadata": {},
   "source": [
    "The performance of the optimized Random Forest model on the oversampled training data is:\n",
    "\n",
    "- Training Accuracy: 95.37%\n",
    "- Training Precision: 92.44%\n",
    "- Training Recall: 99.07%\n",
    "- Training F1-Score: 95.64%\n",
    "\n",
    "The model still performs better on the training set than the test set, indicating the overfitting is still present. However, the gap between training and test performance is now smaller compared to the previous non-optimized model. To further address overfitting, we will try Feature Importance Analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a15c010d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Investigating Feature Importance\n",
    "\n",
    "Understanding which features significantly influence the model's predictions can provide valuable insights and help us with the overfitting problem. To reduce overfitting, we can consider removing features with very low importance since they might be adding noise to the model. By focusing on the most significant features, we can potentially create a simpler model that generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3445c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the optimized model\n",
    "feature_importances = rf_optimized.feature_importances_\n",
    "\n",
    "# Create a DataFrame for the importances and their corresponding features\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X_oversampled.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the features based on importance\n",
    "sorted_features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.barh(sorted_features_df['Feature'], sorted_features_df['Importance'], align='center', alpha=0.8)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n",
    "\n",
    "# Set a threshold for feature importance\n",
    "threshold = 0.01\n",
    "\n",
    "# Select features above the threshold\n",
    "selected_features = sorted_features_df[sorted_features_df['Importance'] > threshold]['Feature'].tolist()\n",
    "\n",
    "# Extract the selected features from the oversampled data\n",
    "X_train_over_selected = X_train_over[:, X_oversampled.columns.isin(selected_features)]\n",
    "X_test_over_selected = X_test_over[:, X_oversampled.columns.isin(selected_features)]\n",
    "\n",
    "# Retrain the Random Forest model using the selected features\n",
    "rf_selected = RandomForestClassifier(**best_params_quick, random_state=42)\n",
    "rf_selected.fit(X_train_over_selected, y_train_over)\n",
    "\n",
    "# Predict and evaluate on the training data\n",
    "rf_selected_train_predictions = rf_selected.predict(X_train_over_selected)\n",
    "rf_selected_train_accuracy = accuracy_score(y_train_over, rf_selected_train_predictions)\n",
    "rf_selected_train_precision = precision_score(y_train_over, rf_selected_train_predictions, pos_label=\"Yes\")\n",
    "rf_selected_train_recall = recall_score(y_train_over, rf_selected_train_predictions, pos_label=\"Yes\")\n",
    "rf_selected_train_f1 = f1_score(y_train_over, rf_selected_train_predictions, pos_label=\"Yes\")\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "rf_selected_test_predictions = rf_selected.predict(X_test_over_selected)\n",
    "rf_selected_test_accuracy = accuracy_score(y_test_over, rf_selected_test_predictions)\n",
    "rf_selected_test_precision = precision_score(y_test_over, rf_selected_test_predictions, pos_label=\"Yes\")\n",
    "rf_selected_test_recall = recall_score(y_test_over, rf_selected_test_predictions, pos_label=\"Yes\")\n",
    "rf_selected_test_f1 = f1_score(y_test_over, rf_selected_test_predictions, pos_label=\"Yes\")\n",
    "\n",
    "rf_selected_train_accuracy, rf_selected_train_precision, rf_selected_train_recall, rf_selected_train_f1, rf_selected_test_accuracy, rf_selected_test_precision, rf_selected_test_recall, rf_selected_test_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aae064d8",
   "metadata": {},
   "source": [
    "After removing the low-importance features and retraining the Random Forest model, here's the performance:\n",
    "\n",
    "On the Training Data (Selected Features):\n",
    "\n",
    "- Training Accuracy: 98.60%\n",
    "- Training Precision: 97.68%\n",
    "- Training Recall: 99.64%\n",
    "- Training F1-Score: 98.65%\n",
    "\n",
    "On the Test Data (Selected Features):\n",
    "\n",
    "- Test Accuracy: 87.39%\n",
    "- Test Precision: 81.95%\n",
    "- Test Recall: 92.56%\n",
    "- Test F1-Score: 86.93%\n",
    "\n",
    "In summary, by focusing on the most important features, we've reduced some of the overfitting and improved the model's performance on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9ef000e",
   "metadata": {},
   "source": [
    "# 3. Bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ab75b9c",
   "metadata": {},
   "source": [
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble method that aims to improve the stability and accuracy of machine learning algorithms. It works by training multiple instances of a model on different subsets of the data (with replacement) and then averaging the predictions. It can potentially help us build a more robust model with reduced overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Create a Bagging classifier with the optimized Random Forest model as the base estimator\n",
    "bagging_rf = BaggingClassifier(base_estimator=rf_selected, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "bagging_rf.fit(X_train_over_selected, y_train_over)\n",
    "\n",
    "# Predict and evaluate on the training data\n",
    "bagging_rf_train_predictions = bagging_rf.predict(X_train_over_selected)\n",
    "bagging_rf_train_accuracy = accuracy_score(y_train_over, bagging_rf_train_predictions)\n",
    "bagging_rf_train_precision = precision_score(y_train_over, bagging_rf_train_predictions, pos_label=\"Yes\")\n",
    "bagging_rf_train_recall = recall_score(y_train_over, bagging_rf_train_predictions, pos_label=\"Yes\")\n",
    "bagging_rf_train_f1 = f1_score(y_train_over, bagging_rf_train_predictions, pos_label=\"Yes\")\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "bagging_rf_test_predictions = bagging_rf.predict(X_test_over_selected)\n",
    "bagging_rf_test_accuracy = accuracy_score(y_test_over, bagging_rf_test_predictions)\n",
    "bagging_rf_test_precision = precision_score(y_test_over, bagging_rf_test_predictions, pos_label=\"Yes\")\n",
    "bagging_rf_test_recall = recall_score(y_test_over, bagging_rf_test_predictions, pos_label=\"Yes\")\n",
    "bagging_rf_test_f1 = f1_score(y_test_over, bagging_rf_test_predictions, pos_label=\"Yes\")\n",
    "\n",
    "bagging_rf_train_accuracy, bagging_rf_train_precision, bagging_rf_train_recall, bagging_rf_train_f1, bagging_rf_test_accuracy, bagging_rf_test_precision, bagging_rf_test_recall, bagging_rf_test_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6be1d38d",
   "metadata": {},
   "source": [
    "After applying Bagging with the Random Forest model on the selected features, here are the results:\n",
    "\n",
    "On the Training Data:\n",
    "\n",
    "- Training Accuracy: 96.15%\n",
    "- Training Precision: 94.98%\n",
    "- Training Recall: 97.63%\n",
    "- Training F1-Score: 96.29%\n",
    "\n",
    "On the Test Data:\n",
    "\n",
    "- Test Accuracy: 84.75%\n",
    "- Test Precision: 79.54%\n",
    "- Test Recall: 89.32%\n",
    "- Test F1-Score: 84.15%\n",
    "\n",
    "The Bagging approach has slightly reduced overfitting, as indicated by a smaller gap between training and test performance. However, the performance on the test set is slightly reduced compared to the previous model. Despite the reduction in test performance, the Bagging approach offers a more robust model, as it averages predictions from multiple bootstrapped datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a50d269",
   "metadata": {},
   "source": [
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Data Exploration and Cleaning:** During the exploratory data analysis phase, we identified that the dataset was imbalanced, with more instances of non-churned customers than churned ones.\n",
    "\n",
    "2. **Modeling:** We started with three baseline models: Logistic Regression, Random Forest, and Gradient Boosting. Random Forest performed the best out of the three initial models.\n",
    "\n",
    "3. **Handling Imbalance:** To address the class imbalance, we explored oversampling and undersampling techniques. Oversampling the minority class resulted in improved model performance, especially in terms of recall. However, model suffered from overfitting.\n",
    "\n",
    "4. **Model Optimization:** To optimize the model, we fine-tuned the parameters, performed the feature importance analysis, and Bagging."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b55c6e5d",
   "metadata": {},
   "source": [
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Further Analysis:** While we achieved significant improvements in our model, there's always room for more advanced techniques like neural networks or ensemble methods that combine various models for potentially better performance.\n",
    "\n",
    "2. **Feature Engineering:** Deriving new features or transforming existing ones can provide the model with additional information that might enhance its predictive capabilities.\n",
    "\n",
    "3. **Feedback Loop:** Once the model is deployed, it's crucial to set up a feedback mechanism to continuously collect actual results and refine the model over time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
